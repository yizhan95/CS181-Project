{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import logging\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import json\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark Version 2.2.0\n",
      "Row(business_id='YDf95gJZaq05wvo7hTQbbQ', name='Richmond Town Square', categories=['Shopping', 'Shopping Centers'])\n",
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "Row(review_id='VfBHSwC5Vz_pbFluy07i9Q', user_id='cjpdDjZyprfyDG3RlkVG3w', business_id='uYHaNptLzDLoV_JZ_MuzUA', text='My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\n\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\n\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.')\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Running Spark Version %s\" % (spark.version))\n",
    "\n",
    "#### load business data ###\n",
    "# path_business =\"yizhan/Desktop/cs181/dataset/buiness.json\"\n",
    "path_business = \"dataset/business.json\"\n",
    "df_business_raw = spark.read.json(path_business)\n",
    "# df_business_raw.first()\n",
    "\n",
    "businessDF = df_business_raw.select(df_business_raw[\"business_id\"], df_business_raw[\"name\"], df_business_raw[\"categories\"])\n",
    "print(businessDF.first())\n",
    "businessDF.printSchema()\n",
    "businessDF.createOrReplaceTempView(\"business\")\n",
    "\n",
    "# path_review=\"yizhan/Desktop/CS181/yelp dataset/review.json\"\n",
    "path_review=\"dataset/review.json\"\n",
    "df_review_raw = spark.read.json(path_review)\n",
    "# df_review_raw.first()\n",
    "\n",
    "reviewDF= df_review_raw.select(df_review_raw[\"review_id\"], df_review_raw[\"user_id\"], df_review_raw[\"business_id\"], df_review_raw[\"text\"])\n",
    "print(reviewDF.first())\n",
    "reviewDF.printSchema()\n",
    "reviewDF.createOrReplaceTempView(\"review\")\n",
    "\n",
    "# join reviews with busines on businnes_id \n",
    "merged_review = spark.sql(\"SELECT * FROM review left outer join business on review.business_id == business.business_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_review.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = merged_review.take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(sample)\n",
    "type(sample)\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate data with multiple features\n",
    "list = []\n",
    "label_to_num_dict = {}\n",
    "num_to_label_dict = {}\n",
    "index = 0 \n",
    "for line in sample:\n",
    "    labels = line[\"categories\"]\n",
    "    text = line[\"text\"]\n",
    "    # print(text)\n",
    "    for label in labels:\n",
    "        if label not in label_to_num_dict:\n",
    "            # print(index, \" represents \",label)\n",
    "            label_to_num_dict[label] = index\n",
    "            num_to_label_dict[index] = label \n",
    "            index += 1  \n",
    "        # covert the category label to numerical value\n",
    "        num_label = label_dict[label]      \n",
    "        list.append([num_label,text])\n",
    "        \n",
    "# print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# convert to rdd \n",
    "sample_with_dup = spark.sparkContext.parallelize(list)\n",
    "label = sample_with_dup.map(lambda item: item[0])\n",
    "text = sample_with_dup.map(lambda item: item[1])\n",
    "\n",
    "'''\n",
    "sampleDF = spark.createDataFrame(list)\n",
    "sampleDF.printSchema()\n",
    "\n",
    "'''\n",
    "schemaString = \"label text\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "sampleDF = spark.createDataFrame(sample_with_dup, schema)\n",
    "sampleDF.printSchema()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sampleDF)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=30)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "'''\n",
    "#labeled_points = sample_with_dup.map(lambda item : LabeledPoint([item[0],item[1]]))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sampleDF)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.transform(wordsData) #.show(truncate=False) \n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000) \n",
    "featurizedData = hashingTF.transform(remover)\n",
    "\n",
    "#hashingTF = HashingTF(numFeatures = 1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData) '''\n",
    "'''\n",
    "tf = hashingTF.transform()\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "'''\n",
    "sampleDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf:\n",
      "Row(label='0', text=\"What can I say.. Wowzers! Probably one of the best steak houses I've been too. Service was absolutely flawless and dinner was excellent . Ordered seafood tower, wedge, wagyu filet, chateaubriand, bacon grits and sautéed  mushrooms Will definitely be back!\", words=['what', 'can', 'i', 'say..', 'wowzers!', 'probably', 'one', 'of', 'the', 'best', 'steak', 'houses', \"i've\", 'been', 'too.', 'service', 'was', 'absolutely', 'flawless', 'and', 'dinner', 'was', 'excellent', '.', 'ordered', 'seafood', 'tower,', 'wedge,', 'wagyu', 'filet,', 'chateaubriand,', 'bacon', 'grits', 'and', 'sautéed', '', 'mushrooms', 'will', 'definitely', 'be', 'back!'], rawFeatures=SparseVector(30, {0: 1.0, 1: 1.0, 2: 1.0, 3: 4.0, 4: 2.0, 8: 2.0, 9: 1.0, 10: 1.0, 11: 1.0, 12: 3.0, 13: 3.0, 14: 3.0, 15: 3.0, 16: 2.0, 17: 1.0, 19: 1.0, 20: 1.0, 21: 1.0, 23: 1.0, 24: 2.0, 26: 3.0, 28: 1.0, 29: 2.0}), features=SparseVector(30, {0: 0.0483, 1: 0.1144, 2: 0.3031, 3: 0.4545, 4: 0.5479, 8: 0.5063, 9: 0.1353, 10: 0.2278, 11: 0.2652, 12: 0.476, 13: 0.0974, 14: 0.4985, 15: 0.8471, 16: 0.4135, 17: 0.1977, 19: 0.3414, 20: 0.0532, 21: 0.2494, 23: 0.1773, 24: 0.4836, 26: 0.5003, 28: 0.0886, 29: 0.4196}))\n",
      "Row(label='1', text=\"What can I say.. Wowzers! Probably one of the best steak houses I've been too. Service was absolutely flawless and dinner was excellent . Ordered seafood tower, wedge, wagyu filet, chateaubriand, bacon grits and sautéed  mushrooms Will definitely be back!\", words=['what', 'can', 'i', 'say..', 'wowzers!', 'probably', 'one', 'of', 'the', 'best', 'steak', 'houses', \"i've\", 'been', 'too.', 'service', 'was', 'absolutely', 'flawless', 'and', 'dinner', 'was', 'excellent', '.', 'ordered', 'seafood', 'tower,', 'wedge,', 'wagyu', 'filet,', 'chateaubriand,', 'bacon', 'grits', 'and', 'sautéed', '', 'mushrooms', 'will', 'definitely', 'be', 'back!'], rawFeatures=SparseVector(30, {0: 1.0, 1: 1.0, 2: 1.0, 3: 4.0, 4: 2.0, 8: 2.0, 9: 1.0, 10: 1.0, 11: 1.0, 12: 3.0, 13: 3.0, 14: 3.0, 15: 3.0, 16: 2.0, 17: 1.0, 19: 1.0, 20: 1.0, 21: 1.0, 23: 1.0, 24: 2.0, 26: 3.0, 28: 1.0, 29: 2.0}), features=SparseVector(30, {0: 0.0483, 1: 0.1144, 2: 0.3031, 3: 0.4545, 4: 0.5479, 8: 0.5063, 9: 0.1353, 10: 0.2278, 11: 0.2652, 12: 0.476, 13: 0.0974, 14: 0.4985, 15: 0.8471, 16: 0.4135, 17: 0.1977, 19: 0.3414, 20: 0.0532, 21: 0.2494, 23: 0.1773, 24: 0.4836, 26: 0.5003, 28: 0.0886, 29: 0.4196}))\n",
      "Row(label='2', text=\"What can I say.. Wowzers! Probably one of the best steak houses I've been too. Service was absolutely flawless and dinner was excellent . Ordered seafood tower, wedge, wagyu filet, chateaubriand, bacon grits and sautéed  mushrooms Will definitely be back!\", words=['what', 'can', 'i', 'say..', 'wowzers!', 'probably', 'one', 'of', 'the', 'best', 'steak', 'houses', \"i've\", 'been', 'too.', 'service', 'was', 'absolutely', 'flawless', 'and', 'dinner', 'was', 'excellent', '.', 'ordered', 'seafood', 'tower,', 'wedge,', 'wagyu', 'filet,', 'chateaubriand,', 'bacon', 'grits', 'and', 'sautéed', '', 'mushrooms', 'will', 'definitely', 'be', 'back!'], rawFeatures=SparseVector(30, {0: 1.0, 1: 1.0, 2: 1.0, 3: 4.0, 4: 2.0, 8: 2.0, 9: 1.0, 10: 1.0, 11: 1.0, 12: 3.0, 13: 3.0, 14: 3.0, 15: 3.0, 16: 2.0, 17: 1.0, 19: 1.0, 20: 1.0, 21: 1.0, 23: 1.0, 24: 2.0, 26: 3.0, 28: 1.0, 29: 2.0}), features=SparseVector(30, {0: 0.0483, 1: 0.1144, 2: 0.3031, 3: 0.4545, 4: 0.5479, 8: 0.5063, 9: 0.1353, 10: 0.2278, 11: 0.2652, 12: 0.476, 13: 0.0974, 14: 0.4985, 15: 0.8471, 16: 0.4135, 17: 0.1977, 19: 0.3414, 20: 0.0532, 21: 0.2494, 23: 0.1773, 24: 0.4836, 26: 0.5003, 28: 0.0886, 29: 0.4196}))\n",
      "Row(label='0', text='We went in with high hopes after a friend recommended Delmonico but we left just a bit disappointed.  The ambiance was really nice and we were given a great table without a reservation but the food left a little to be desired.  \\n\\nMy friend and I both started with lobster bisque, which was good but a little thin.  We noticed right away too that she had 2 tiny little pieces of lobster in hers, but I had almost an entire small claw.  I was happy until I realized it was tough and rubbery.  I actually had to use a knife and fork to cut it into bite sized pieces.  My husband ordered the wedge salad as a starter and it was 80% white/yellow core.  Not what you would expect in a place as pricey as this.\\n\\nI ordered the fillet and it was cooked perfectly but it was not a great cut of meat.  Maybe I have high standards, but I think a $30+ fillet should be butter knife soft, but this one was kind of fibrous and hard to cut with a steak knife.  I didn\\'t really care for the seasoning either, but that\\'s a personal preference, not something I can count against the restaurant.  My husband\\'s rib eye was really fatty but he loved his twice baked potato.  I had the corn gratin for my side and it was tasty but again, not my style.  \\n\\nOur server was efficient but pretty standoffish.  When we first sat down, we ordered a bottle of \"Emerel\\'s Blend\" wine and he nicely suggested that we might want to try something else, but other than that, his interaction with us was a bit cold.  \\n\\nMaybe it was an off night, but I really expected more from a $400+ dinner.', words=['we', 'went', 'in', 'with', 'high', 'hopes', 'after', 'a', 'friend', 'recommended', 'delmonico', 'but', 'we', 'left', 'just', 'a', 'bit', 'disappointed.', '', 'the', 'ambiance', 'was', 'really', 'nice', 'and', 'we', 'were', 'given', 'a', 'great', 'table', 'without', 'a', 'reservation', 'but', 'the', 'food', 'left', 'a', 'little', 'to', 'be', 'desired.', '', '', '', 'my', 'friend', 'and', 'i', 'both', 'started', 'with', 'lobster', 'bisque,', 'which', 'was', 'good', 'but', 'a', 'little', 'thin.', '', 'we', 'noticed', 'right', 'away', 'too', 'that', 'she', 'had', '2', 'tiny', 'little', 'pieces', 'of', 'lobster', 'in', 'hers,', 'but', 'i', 'had', 'almost', 'an', 'entire', 'small', 'claw.', '', 'i', 'was', 'happy', 'until', 'i', 'realized', 'it', 'was', 'tough', 'and', 'rubbery.', '', 'i', 'actually', 'had', 'to', 'use', 'a', 'knife', 'and', 'fork', 'to', 'cut', 'it', 'into', 'bite', 'sized', 'pieces.', '', 'my', 'husband', 'ordered', 'the', 'wedge', 'salad', 'as', 'a', 'starter', 'and', 'it', 'was', '80%', 'white/yellow', 'core.', '', 'not', 'what', 'you', 'would', 'expect', 'in', 'a', 'place', 'as', 'pricey', 'as', 'this.', '', 'i', 'ordered', 'the', 'fillet', 'and', 'it', 'was', 'cooked', 'perfectly', 'but', 'it', 'was', 'not', 'a', 'great', 'cut', 'of', 'meat.', '', 'maybe', 'i', 'have', 'high', 'standards,', 'but', 'i', 'think', 'a', '$30+', 'fillet', 'should', 'be', 'butter', 'knife', 'soft,', 'but', 'this', 'one', 'was', 'kind', 'of', 'fibrous', 'and', 'hard', 'to', 'cut', 'with', 'a', 'steak', 'knife.', '', 'i', \"didn't\", 'really', 'care', 'for', 'the', 'seasoning', 'either,', 'but', \"that's\", 'a', 'personal', 'preference,', 'not', 'something', 'i', 'can', 'count', 'against', 'the', 'restaurant.', '', 'my', \"husband's\", 'rib', 'eye', 'was', 'really', 'fatty', 'but', 'he', 'loved', 'his', 'twice', 'baked', 'potato.', '', 'i', 'had', 'the', 'corn', 'gratin', 'for', 'my', 'side', 'and', 'it', 'was', 'tasty', 'but', 'again,', 'not', 'my', 'style.', '', '', '', 'our', 'server', 'was', 'efficient', 'but', 'pretty', 'standoffish.', '', 'when', 'we', 'first', 'sat', 'down,', 'we', 'ordered', 'a', 'bottle', 'of', '\"emerel\\'s', 'blend\"', 'wine', 'and', 'he', 'nicely', 'suggested', 'that', 'we', 'might', 'want', 'to', 'try', 'something', 'else,', 'but', 'other', 'than', 'that,', 'his', 'interaction', 'with', 'us', 'was', 'a', 'bit', 'cold.', '', '', '', 'maybe', 'it', 'was', 'an', 'off', 'night,', 'but', 'i', 'really', 'expected', 'more', 'from', 'a', '$400+', 'dinner.'], rawFeatures=SparseVector(30, {0: 12.0, 1: 8.0, 2: 7.0, 3: 15.0, 4: 6.0, 5: 12.0, 6: 9.0, 7: 4.0, 8: 5.0, 9: 13.0, 10: 6.0, 11: 6.0, 12: 32.0, 13: 33.0, 14: 18.0, 15: 7.0, 16: 7.0, 17: 7.0, 18: 9.0, 19: 1.0, 20: 29.0, 21: 7.0, 22: 5.0, 23: 6.0, 24: 9.0, 25: 1.0, 26: 8.0, 27: 5.0, 28: 19.0, 29: 11.0}), features=SparseVector(30, {0: 0.58, 1: 0.9155, 2: 2.1218, 3: 1.7042, 4: 1.6436, 5: 1.2483, 6: 2.3498, 7: 0.9212, 8: 1.2657, 9: 1.7592, 10: 1.3669, 11: 1.5915, 12: 5.0773, 13: 1.0715, 14: 2.9913, 15: 1.9766, 16: 1.4473, 17: 1.3841, 18: 1.984, 19: 0.3414, 20: 1.5442, 21: 1.7455, 22: 1.3568, 23: 1.0636, 24: 2.1763, 25: 0.3185, 26: 1.3341, 27: 1.323, 28: 1.6834, 29: 2.3076}))\n",
      "Row(label='1', text='We went in with high hopes after a friend recommended Delmonico but we left just a bit disappointed.  The ambiance was really nice and we were given a great table without a reservation but the food left a little to be desired.  \\n\\nMy friend and I both started with lobster bisque, which was good but a little thin.  We noticed right away too that she had 2 tiny little pieces of lobster in hers, but I had almost an entire small claw.  I was happy until I realized it was tough and rubbery.  I actually had to use a knife and fork to cut it into bite sized pieces.  My husband ordered the wedge salad as a starter and it was 80% white/yellow core.  Not what you would expect in a place as pricey as this.\\n\\nI ordered the fillet and it was cooked perfectly but it was not a great cut of meat.  Maybe I have high standards, but I think a $30+ fillet should be butter knife soft, but this one was kind of fibrous and hard to cut with a steak knife.  I didn\\'t really care for the seasoning either, but that\\'s a personal preference, not something I can count against the restaurant.  My husband\\'s rib eye was really fatty but he loved his twice baked potato.  I had the corn gratin for my side and it was tasty but again, not my style.  \\n\\nOur server was efficient but pretty standoffish.  When we first sat down, we ordered a bottle of \"Emerel\\'s Blend\" wine and he nicely suggested that we might want to try something else, but other than that, his interaction with us was a bit cold.  \\n\\nMaybe it was an off night, but I really expected more from a $400+ dinner.', words=['we', 'went', 'in', 'with', 'high', 'hopes', 'after', 'a', 'friend', 'recommended', 'delmonico', 'but', 'we', 'left', 'just', 'a', 'bit', 'disappointed.', '', 'the', 'ambiance', 'was', 'really', 'nice', 'and', 'we', 'were', 'given', 'a', 'great', 'table', 'without', 'a', 'reservation', 'but', 'the', 'food', 'left', 'a', 'little', 'to', 'be', 'desired.', '', '', '', 'my', 'friend', 'and', 'i', 'both', 'started', 'with', 'lobster', 'bisque,', 'which', 'was', 'good', 'but', 'a', 'little', 'thin.', '', 'we', 'noticed', 'right', 'away', 'too', 'that', 'she', 'had', '2', 'tiny', 'little', 'pieces', 'of', 'lobster', 'in', 'hers,', 'but', 'i', 'had', 'almost', 'an', 'entire', 'small', 'claw.', '', 'i', 'was', 'happy', 'until', 'i', 'realized', 'it', 'was', 'tough', 'and', 'rubbery.', '', 'i', 'actually', 'had', 'to', 'use', 'a', 'knife', 'and', 'fork', 'to', 'cut', 'it', 'into', 'bite', 'sized', 'pieces.', '', 'my', 'husband', 'ordered', 'the', 'wedge', 'salad', 'as', 'a', 'starter', 'and', 'it', 'was', '80%', 'white/yellow', 'core.', '', 'not', 'what', 'you', 'would', 'expect', 'in', 'a', 'place', 'as', 'pricey', 'as', 'this.', '', 'i', 'ordered', 'the', 'fillet', 'and', 'it', 'was', 'cooked', 'perfectly', 'but', 'it', 'was', 'not', 'a', 'great', 'cut', 'of', 'meat.', '', 'maybe', 'i', 'have', 'high', 'standards,', 'but', 'i', 'think', 'a', '$30+', 'fillet', 'should', 'be', 'butter', 'knife', 'soft,', 'but', 'this', 'one', 'was', 'kind', 'of', 'fibrous', 'and', 'hard', 'to', 'cut', 'with', 'a', 'steak', 'knife.', '', 'i', \"didn't\", 'really', 'care', 'for', 'the', 'seasoning', 'either,', 'but', \"that's\", 'a', 'personal', 'preference,', 'not', 'something', 'i', 'can', 'count', 'against', 'the', 'restaurant.', '', 'my', \"husband's\", 'rib', 'eye', 'was', 'really', 'fatty', 'but', 'he', 'loved', 'his', 'twice', 'baked', 'potato.', '', 'i', 'had', 'the', 'corn', 'gratin', 'for', 'my', 'side', 'and', 'it', 'was', 'tasty', 'but', 'again,', 'not', 'my', 'style.', '', '', '', 'our', 'server', 'was', 'efficient', 'but', 'pretty', 'standoffish.', '', 'when', 'we', 'first', 'sat', 'down,', 'we', 'ordered', 'a', 'bottle', 'of', '\"emerel\\'s', 'blend\"', 'wine', 'and', 'he', 'nicely', 'suggested', 'that', 'we', 'might', 'want', 'to', 'try', 'something', 'else,', 'but', 'other', 'than', 'that,', 'his', 'interaction', 'with', 'us', 'was', 'a', 'bit', 'cold.', '', '', '', 'maybe', 'it', 'was', 'an', 'off', 'night,', 'but', 'i', 'really', 'expected', 'more', 'from', 'a', '$400+', 'dinner.'], rawFeatures=SparseVector(30, {0: 12.0, 1: 8.0, 2: 7.0, 3: 15.0, 4: 6.0, 5: 12.0, 6: 9.0, 7: 4.0, 8: 5.0, 9: 13.0, 10: 6.0, 11: 6.0, 12: 32.0, 13: 33.0, 14: 18.0, 15: 7.0, 16: 7.0, 17: 7.0, 18: 9.0, 19: 1.0, 20: 29.0, 21: 7.0, 22: 5.0, 23: 6.0, 24: 9.0, 25: 1.0, 26: 8.0, 27: 5.0, 28: 19.0, 29: 11.0}), features=SparseVector(30, {0: 0.58, 1: 0.9155, 2: 2.1218, 3: 1.7042, 4: 1.6436, 5: 1.2483, 6: 2.3498, 7: 0.9212, 8: 1.2657, 9: 1.7592, 10: 1.3669, 11: 1.5915, 12: 5.0773, 13: 1.0715, 14: 2.9913, 15: 1.9766, 16: 1.4473, 17: 1.3841, 18: 1.984, 19: 0.3414, 20: 1.5442, 21: 1.7455, 22: 1.3568, 23: 1.0636, 24: 2.1763, 25: 0.3185, 26: 1.3341, 27: 1.323, 28: 1.6834, 29: 2.3076}))\n",
      "Row(label='2', text='We went in with high hopes after a friend recommended Delmonico but we left just a bit disappointed.  The ambiance was really nice and we were given a great table without a reservation but the food left a little to be desired.  \\n\\nMy friend and I both started with lobster bisque, which was good but a little thin.  We noticed right away too that she had 2 tiny little pieces of lobster in hers, but I had almost an entire small claw.  I was happy until I realized it was tough and rubbery.  I actually had to use a knife and fork to cut it into bite sized pieces.  My husband ordered the wedge salad as a starter and it was 80% white/yellow core.  Not what you would expect in a place as pricey as this.\\n\\nI ordered the fillet and it was cooked perfectly but it was not a great cut of meat.  Maybe I have high standards, but I think a $30+ fillet should be butter knife soft, but this one was kind of fibrous and hard to cut with a steak knife.  I didn\\'t really care for the seasoning either, but that\\'s a personal preference, not something I can count against the restaurant.  My husband\\'s rib eye was really fatty but he loved his twice baked potato.  I had the corn gratin for my side and it was tasty but again, not my style.  \\n\\nOur server was efficient but pretty standoffish.  When we first sat down, we ordered a bottle of \"Emerel\\'s Blend\" wine and he nicely suggested that we might want to try something else, but other than that, his interaction with us was a bit cold.  \\n\\nMaybe it was an off night, but I really expected more from a $400+ dinner.', words=['we', 'went', 'in', 'with', 'high', 'hopes', 'after', 'a', 'friend', 'recommended', 'delmonico', 'but', 'we', 'left', 'just', 'a', 'bit', 'disappointed.', '', 'the', 'ambiance', 'was', 'really', 'nice', 'and', 'we', 'were', 'given', 'a', 'great', 'table', 'without', 'a', 'reservation', 'but', 'the', 'food', 'left', 'a', 'little', 'to', 'be', 'desired.', '', '', '', 'my', 'friend', 'and', 'i', 'both', 'started', 'with', 'lobster', 'bisque,', 'which', 'was', 'good', 'but', 'a', 'little', 'thin.', '', 'we', 'noticed', 'right', 'away', 'too', 'that', 'she', 'had', '2', 'tiny', 'little', 'pieces', 'of', 'lobster', 'in', 'hers,', 'but', 'i', 'had', 'almost', 'an', 'entire', 'small', 'claw.', '', 'i', 'was', 'happy', 'until', 'i', 'realized', 'it', 'was', 'tough', 'and', 'rubbery.', '', 'i', 'actually', 'had', 'to', 'use', 'a', 'knife', 'and', 'fork', 'to', 'cut', 'it', 'into', 'bite', 'sized', 'pieces.', '', 'my', 'husband', 'ordered', 'the', 'wedge', 'salad', 'as', 'a', 'starter', 'and', 'it', 'was', '80%', 'white/yellow', 'core.', '', 'not', 'what', 'you', 'would', 'expect', 'in', 'a', 'place', 'as', 'pricey', 'as', 'this.', '', 'i', 'ordered', 'the', 'fillet', 'and', 'it', 'was', 'cooked', 'perfectly', 'but', 'it', 'was', 'not', 'a', 'great', 'cut', 'of', 'meat.', '', 'maybe', 'i', 'have', 'high', 'standards,', 'but', 'i', 'think', 'a', '$30+', 'fillet', 'should', 'be', 'butter', 'knife', 'soft,', 'but', 'this', 'one', 'was', 'kind', 'of', 'fibrous', 'and', 'hard', 'to', 'cut', 'with', 'a', 'steak', 'knife.', '', 'i', \"didn't\", 'really', 'care', 'for', 'the', 'seasoning', 'either,', 'but', \"that's\", 'a', 'personal', 'preference,', 'not', 'something', 'i', 'can', 'count', 'against', 'the', 'restaurant.', '', 'my', \"husband's\", 'rib', 'eye', 'was', 'really', 'fatty', 'but', 'he', 'loved', 'his', 'twice', 'baked', 'potato.', '', 'i', 'had', 'the', 'corn', 'gratin', 'for', 'my', 'side', 'and', 'it', 'was', 'tasty', 'but', 'again,', 'not', 'my', 'style.', '', '', '', 'our', 'server', 'was', 'efficient', 'but', 'pretty', 'standoffish.', '', 'when', 'we', 'first', 'sat', 'down,', 'we', 'ordered', 'a', 'bottle', 'of', '\"emerel\\'s', 'blend\"', 'wine', 'and', 'he', 'nicely', 'suggested', 'that', 'we', 'might', 'want', 'to', 'try', 'something', 'else,', 'but', 'other', 'than', 'that,', 'his', 'interaction', 'with', 'us', 'was', 'a', 'bit', 'cold.', '', '', '', 'maybe', 'it', 'was', 'an', 'off', 'night,', 'but', 'i', 'really', 'expected', 'more', 'from', 'a', '$400+', 'dinner.'], rawFeatures=SparseVector(30, {0: 12.0, 1: 8.0, 2: 7.0, 3: 15.0, 4: 6.0, 5: 12.0, 6: 9.0, 7: 4.0, 8: 5.0, 9: 13.0, 10: 6.0, 11: 6.0, 12: 32.0, 13: 33.0, 14: 18.0, 15: 7.0, 16: 7.0, 17: 7.0, 18: 9.0, 19: 1.0, 20: 29.0, 21: 7.0, 22: 5.0, 23: 6.0, 24: 9.0, 25: 1.0, 26: 8.0, 27: 5.0, 28: 19.0, 29: 11.0}), features=SparseVector(30, {0: 0.58, 1: 0.9155, 2: 2.1218, 3: 1.7042, 4: 1.6436, 5: 1.2483, 6: 2.3498, 7: 0.9212, 8: 1.2657, 9: 1.7592, 10: 1.3669, 11: 1.5915, 12: 5.0773, 13: 1.0715, 14: 2.9913, 15: 1.9766, 16: 1.4473, 17: 1.3841, 18: 1.984, 19: 0.3414, 20: 1.5442, 21: 1.7455, 22: 1.3568, 23: 1.0636, 24: 2.1763, 25: 0.3185, 26: 1.3341, 27: 1.323, 28: 1.6834, 29: 2.3076}))\n",
      "Row(label='0', text='I want to give this place 2 stars, but because of the excellent service I\\'ll bump it up to 3. The best thing about this place was the impeccable service received.  I dined here with my husband on a late Friday night.  The food here was just \"OK\". My breakdown of each dish is this:\\n\\nThe Popovers: meh..they were ok. I think they were over cooked which made them a bit too dry and with a slightly burnt butter taste. I like Tyler Florence does a better version at Wayfare Tavern (SF).\\n\\nAppetizer special - Pappardelle w/ Crimini Mushrooms in a white wine sauce finished with parmigiana: Good stuff. Very light and tasty pasta, too many mushrooms for my liking, I would\\'ve preferred more pasta. Very small tasting plate so don\\'t expect to get more than 3 or 4 bites.\\n\\nCreamed spinach: Another meh dish. I wanted more flavor here. Maybe a finish with butter would\\'ve helped. Some seasoning or something. Literally all I could taste was watery spinach.\\n\\nTwice Baled loaded Potatoes: YESSSSSS PLEASE! Now this was a great dish. Flavor in every bite. Perfect partner for my steak.\\n\\nRibeye Steak (w/o bone) OSCAR STYLE w/ Jumbo Lump Crab Meat, Grilled Asparagus and Béarnaise.- I SOOOOOO wanted to love this steak. I really did. Order a my steak cooked medium and unfortunately it came out damn near rare. Like literally just short of bleeding. There was hardly any flavor.  The best part was the burnt edges as they actually added some flavor. I\\'d give my steak a 2/5, however, the Jumbo Lump crab and Béarnaise WAS THE BOMB!\\n\\nSmoked Pork chop: my husband truly enjoyed his pork chop. Tender with a smokey flavor all the way thru. The pork chop was huge! I\\'m not a big pork chop eater, but it was tasty. \\n\\nPecan pie: Now this was worth the price of admission. A tasty warm & nutty pie with cold delicious ice cream. Mmmmm mmmmm good. I\\'d come here again just to eat this. A MUST have if you\\'re ordering desserts.\\n\\nAs previously mentioned in other reviews, the wine here is definitely over priced. I understand the mark ups restaurants need to make, but I think Delmonico\\'s takes the \"upcharge\" to an entirely new level. We ordered a bottle of Kim Crawford\\'s Sauvignon Blanc, which retails for about $18-20, we were charged $45 for the bottle!!! Talk about a rip off! Outside of opening the bottle and pouring the wine, what constitutes a $25 mark up?\\n\\nNeedless to say I most likely won\\'t return here. I\\'m glad I tried it, but I don\\'t think it was worth the price tag. I\\'d prefer to go back to Mario Batali\\'s B&B, which is located directly across the walk way from Delmonico\\'s.', words=['i', 'want', 'to', 'give', 'this', 'place', '2', 'stars,', 'but', 'because', 'of', 'the', 'excellent', 'service', \"i'll\", 'bump', 'it', 'up', 'to', '3.', 'the', 'best', 'thing', 'about', 'this', 'place', 'was', 'the', 'impeccable', 'service', 'received.', '', 'i', 'dined', 'here', 'with', 'my', 'husband', 'on', 'a', 'late', 'friday', 'night.', '', 'the', 'food', 'here', 'was', 'just', '\"ok\".', 'my', 'breakdown', 'of', 'each', 'dish', 'is', 'this:', '', 'the', 'popovers:', 'meh..they', 'were', 'ok.', 'i', 'think', 'they', 'were', 'over', 'cooked', 'which', 'made', 'them', 'a', 'bit', 'too', 'dry', 'and', 'with', 'a', 'slightly', 'burnt', 'butter', 'taste.', 'i', 'like', 'tyler', 'florence', 'does', 'a', 'better', 'version', 'at', 'wayfare', 'tavern', '(sf).', '', 'appetizer', 'special', '-', 'pappardelle', 'w/', 'crimini', 'mushrooms', 'in', 'a', 'white', 'wine', 'sauce', 'finished', 'with', 'parmigiana:', 'good', 'stuff.', 'very', 'light', 'and', 'tasty', 'pasta,', 'too', 'many', 'mushrooms', 'for', 'my', 'liking,', 'i', \"would've\", 'preferred', 'more', 'pasta.', 'very', 'small', 'tasting', 'plate', 'so', \"don't\", 'expect', 'to', 'get', 'more', 'than', '3', 'or', '4', 'bites.', '', 'creamed', 'spinach:', 'another', 'meh', 'dish.', 'i', 'wanted', 'more', 'flavor', 'here.', 'maybe', 'a', 'finish', 'with', 'butter', \"would've\", 'helped.', 'some', 'seasoning', 'or', 'something.', 'literally', 'all', 'i', 'could', 'taste', 'was', 'watery', 'spinach.', '', 'twice', 'baled', 'loaded', 'potatoes:', 'yessssss', 'please!', 'now', 'this', 'was', 'a', 'great', 'dish.', 'flavor', 'in', 'every', 'bite.', 'perfect', 'partner', 'for', 'my', 'steak.', '', 'ribeye', 'steak', '(w/o', 'bone)', 'oscar', 'style', 'w/', 'jumbo', 'lump', 'crab', 'meat,', 'grilled', 'asparagus', 'and', 'béarnaise.-', 'i', 'soooooo', 'wanted', 'to', 'love', 'this', 'steak.', 'i', 'really', 'did.', 'order', 'a', 'my', 'steak', 'cooked', 'medium', 'and', 'unfortunately', 'it', 'came', 'out', 'damn', 'near', 'rare.', 'like', 'literally', 'just', 'short', 'of', 'bleeding.', 'there', 'was', 'hardly', 'any', 'flavor.', '', 'the', 'best', 'part', 'was', 'the', 'burnt', 'edges', 'as', 'they', 'actually', 'added', 'some', 'flavor.', \"i'd\", 'give', 'my', 'steak', 'a', '2/5,', 'however,', 'the', 'jumbo', 'lump', 'crab', 'and', 'béarnaise', 'was', 'the', 'bomb!', '', 'smoked', 'pork', 'chop:', 'my', 'husband', 'truly', 'enjoyed', 'his', 'pork', 'chop.', 'tender', 'with', 'a', 'smokey', 'flavor', 'all', 'the', 'way', 'thru.', 'the', 'pork', 'chop', 'was', 'huge!', \"i'm\", 'not', 'a', 'big', 'pork', 'chop', 'eater,', 'but', 'it', 'was', 'tasty.', '', '', 'pecan', 'pie:', 'now', 'this', 'was', 'worth', 'the', 'price', 'of', 'admission.', 'a', 'tasty', 'warm', '&', 'nutty', 'pie', 'with', 'cold', 'delicious', 'ice', 'cream.', 'mmmmm', 'mmmmm', 'good.', \"i'd\", 'come', 'here', 'again', 'just', 'to', 'eat', 'this.', 'a', 'must', 'have', 'if', \"you're\", 'ordering', 'desserts.', '', 'as', 'previously', 'mentioned', 'in', 'other', 'reviews,', 'the', 'wine', 'here', 'is', 'definitely', 'over', 'priced.', 'i', 'understand', 'the', 'mark', 'ups', 'restaurants', 'need', 'to', 'make,', 'but', 'i', 'think', \"delmonico's\", 'takes', 'the', '\"upcharge\"', 'to', 'an', 'entirely', 'new', 'level.', 'we', 'ordered', 'a', 'bottle', 'of', 'kim', \"crawford's\", 'sauvignon', 'blanc,', 'which', 'retails', 'for', 'about', '$18-20,', 'we', 'were', 'charged', '$45', 'for', 'the', 'bottle!!!', 'talk', 'about', 'a', 'rip', 'off!', 'outside', 'of', 'opening', 'the', 'bottle', 'and', 'pouring', 'the', 'wine,', 'what', 'constitutes', 'a', '$25', 'mark', 'up?', '', 'needless', 'to', 'say', 'i', 'most', 'likely', \"won't\", 'return', 'here.', \"i'm\", 'glad', 'i', 'tried', 'it,', 'but', 'i', \"don't\", 'think', 'it', 'was', 'worth', 'the', 'price', 'tag.', \"i'd\", 'prefer', 'to', 'go', 'back', 'to', 'mario', \"batali's\", 'b&b,', 'which', 'is', 'located', 'directly', 'across', 'the', 'walk', 'way', 'from', \"delmonico's.\"], rawFeatures=SparseVector(30, {0: 31.0, 1: 14.0, 2: 12.0, 3: 16.0, 4: 14.0, 5: 14.0, 6: 7.0, 7: 10.0, 8: 8.0, 9: 25.0, 10: 9.0, 11: 6.0, 12: 23.0, 13: 26.0, 14: 30.0, 15: 18.0, 16: 10.0, 17: 15.0, 18: 14.0, 19: 9.0, 20: 31.0, 21: 12.0, 22: 11.0, 23: 10.0, 24: 15.0, 25: 7.0, 26: 18.0, 27: 11.0, 28: 24.0, 29: 24.0}), features=SparseVector(30, {0: 1.4984, 1: 1.6022, 2: 3.6374, 3: 1.8178, 4: 3.8351, 5: 1.4563, 6: 1.8276, 7: 2.303, 8: 2.0251, 9: 3.3831, 10: 2.0504, 11: 1.5915, 12: 3.6493, 13: 0.8442, 14: 4.9855, 15: 5.0827, 16: 2.0675, 17: 2.9659, 18: 3.0862, 19: 3.073, 20: 1.6507, 21: 2.9922, 22: 2.9849, 23: 1.7727, 24: 3.6272, 25: 2.2298, 26: 3.0017, 27: 2.9106, 28: 2.1264, 29: 5.0347}))\n",
      "Row(label='1', text='I want to give this place 2 stars, but because of the excellent service I\\'ll bump it up to 3. The best thing about this place was the impeccable service received.  I dined here with my husband on a late Friday night.  The food here was just \"OK\". My breakdown of each dish is this:\\n\\nThe Popovers: meh..they were ok. I think they were over cooked which made them a bit too dry and with a slightly burnt butter taste. I like Tyler Florence does a better version at Wayfare Tavern (SF).\\n\\nAppetizer special - Pappardelle w/ Crimini Mushrooms in a white wine sauce finished with parmigiana: Good stuff. Very light and tasty pasta, too many mushrooms for my liking, I would\\'ve preferred more pasta. Very small tasting plate so don\\'t expect to get more than 3 or 4 bites.\\n\\nCreamed spinach: Another meh dish. I wanted more flavor here. Maybe a finish with butter would\\'ve helped. Some seasoning or something. Literally all I could taste was watery spinach.\\n\\nTwice Baled loaded Potatoes: YESSSSSS PLEASE! Now this was a great dish. Flavor in every bite. Perfect partner for my steak.\\n\\nRibeye Steak (w/o bone) OSCAR STYLE w/ Jumbo Lump Crab Meat, Grilled Asparagus and Béarnaise.- I SOOOOOO wanted to love this steak. I really did. Order a my steak cooked medium and unfortunately it came out damn near rare. Like literally just short of bleeding. There was hardly any flavor.  The best part was the burnt edges as they actually added some flavor. I\\'d give my steak a 2/5, however, the Jumbo Lump crab and Béarnaise WAS THE BOMB!\\n\\nSmoked Pork chop: my husband truly enjoyed his pork chop. Tender with a smokey flavor all the way thru. The pork chop was huge! I\\'m not a big pork chop eater, but it was tasty. \\n\\nPecan pie: Now this was worth the price of admission. A tasty warm & nutty pie with cold delicious ice cream. Mmmmm mmmmm good. I\\'d come here again just to eat this. A MUST have if you\\'re ordering desserts.\\n\\nAs previously mentioned in other reviews, the wine here is definitely over priced. I understand the mark ups restaurants need to make, but I think Delmonico\\'s takes the \"upcharge\" to an entirely new level. We ordered a bottle of Kim Crawford\\'s Sauvignon Blanc, which retails for about $18-20, we were charged $45 for the bottle!!! Talk about a rip off! Outside of opening the bottle and pouring the wine, what constitutes a $25 mark up?\\n\\nNeedless to say I most likely won\\'t return here. I\\'m glad I tried it, but I don\\'t think it was worth the price tag. I\\'d prefer to go back to Mario Batali\\'s B&B, which is located directly across the walk way from Delmonico\\'s.', words=['i', 'want', 'to', 'give', 'this', 'place', '2', 'stars,', 'but', 'because', 'of', 'the', 'excellent', 'service', \"i'll\", 'bump', 'it', 'up', 'to', '3.', 'the', 'best', 'thing', 'about', 'this', 'place', 'was', 'the', 'impeccable', 'service', 'received.', '', 'i', 'dined', 'here', 'with', 'my', 'husband', 'on', 'a', 'late', 'friday', 'night.', '', 'the', 'food', 'here', 'was', 'just', '\"ok\".', 'my', 'breakdown', 'of', 'each', 'dish', 'is', 'this:', '', 'the', 'popovers:', 'meh..they', 'were', 'ok.', 'i', 'think', 'they', 'were', 'over', 'cooked', 'which', 'made', 'them', 'a', 'bit', 'too', 'dry', 'and', 'with', 'a', 'slightly', 'burnt', 'butter', 'taste.', 'i', 'like', 'tyler', 'florence', 'does', 'a', 'better', 'version', 'at', 'wayfare', 'tavern', '(sf).', '', 'appetizer', 'special', '-', 'pappardelle', 'w/', 'crimini', 'mushrooms', 'in', 'a', 'white', 'wine', 'sauce', 'finished', 'with', 'parmigiana:', 'good', 'stuff.', 'very', 'light', 'and', 'tasty', 'pasta,', 'too', 'many', 'mushrooms', 'for', 'my', 'liking,', 'i', \"would've\", 'preferred', 'more', 'pasta.', 'very', 'small', 'tasting', 'plate', 'so', \"don't\", 'expect', 'to', 'get', 'more', 'than', '3', 'or', '4', 'bites.', '', 'creamed', 'spinach:', 'another', 'meh', 'dish.', 'i', 'wanted', 'more', 'flavor', 'here.', 'maybe', 'a', 'finish', 'with', 'butter', \"would've\", 'helped.', 'some', 'seasoning', 'or', 'something.', 'literally', 'all', 'i', 'could', 'taste', 'was', 'watery', 'spinach.', '', 'twice', 'baled', 'loaded', 'potatoes:', 'yessssss', 'please!', 'now', 'this', 'was', 'a', 'great', 'dish.', 'flavor', 'in', 'every', 'bite.', 'perfect', 'partner', 'for', 'my', 'steak.', '', 'ribeye', 'steak', '(w/o', 'bone)', 'oscar', 'style', 'w/', 'jumbo', 'lump', 'crab', 'meat,', 'grilled', 'asparagus', 'and', 'béarnaise.-', 'i', 'soooooo', 'wanted', 'to', 'love', 'this', 'steak.', 'i', 'really', 'did.', 'order', 'a', 'my', 'steak', 'cooked', 'medium', 'and', 'unfortunately', 'it', 'came', 'out', 'damn', 'near', 'rare.', 'like', 'literally', 'just', 'short', 'of', 'bleeding.', 'there', 'was', 'hardly', 'any', 'flavor.', '', 'the', 'best', 'part', 'was', 'the', 'burnt', 'edges', 'as', 'they', 'actually', 'added', 'some', 'flavor.', \"i'd\", 'give', 'my', 'steak', 'a', '2/5,', 'however,', 'the', 'jumbo', 'lump', 'crab', 'and', 'béarnaise', 'was', 'the', 'bomb!', '', 'smoked', 'pork', 'chop:', 'my', 'husband', 'truly', 'enjoyed', 'his', 'pork', 'chop.', 'tender', 'with', 'a', 'smokey', 'flavor', 'all', 'the', 'way', 'thru.', 'the', 'pork', 'chop', 'was', 'huge!', \"i'm\", 'not', 'a', 'big', 'pork', 'chop', 'eater,', 'but', 'it', 'was', 'tasty.', '', '', 'pecan', 'pie:', 'now', 'this', 'was', 'worth', 'the', 'price', 'of', 'admission.', 'a', 'tasty', 'warm', '&', 'nutty', 'pie', 'with', 'cold', 'delicious', 'ice', 'cream.', 'mmmmm', 'mmmmm', 'good.', \"i'd\", 'come', 'here', 'again', 'just', 'to', 'eat', 'this.', 'a', 'must', 'have', 'if', \"you're\", 'ordering', 'desserts.', '', 'as', 'previously', 'mentioned', 'in', 'other', 'reviews,', 'the', 'wine', 'here', 'is', 'definitely', 'over', 'priced.', 'i', 'understand', 'the', 'mark', 'ups', 'restaurants', 'need', 'to', 'make,', 'but', 'i', 'think', \"delmonico's\", 'takes', 'the', '\"upcharge\"', 'to', 'an', 'entirely', 'new', 'level.', 'we', 'ordered', 'a', 'bottle', 'of', 'kim', \"crawford's\", 'sauvignon', 'blanc,', 'which', 'retails', 'for', 'about', '$18-20,', 'we', 'were', 'charged', '$45', 'for', 'the', 'bottle!!!', 'talk', 'about', 'a', 'rip', 'off!', 'outside', 'of', 'opening', 'the', 'bottle', 'and', 'pouring', 'the', 'wine,', 'what', 'constitutes', 'a', '$25', 'mark', 'up?', '', 'needless', 'to', 'say', 'i', 'most', 'likely', \"won't\", 'return', 'here.', \"i'm\", 'glad', 'i', 'tried', 'it,', 'but', 'i', \"don't\", 'think', 'it', 'was', 'worth', 'the', 'price', 'tag.', \"i'd\", 'prefer', 'to', 'go', 'back', 'to', 'mario', \"batali's\", 'b&b,', 'which', 'is', 'located', 'directly', 'across', 'the', 'walk', 'way', 'from', \"delmonico's.\"], rawFeatures=SparseVector(30, {0: 31.0, 1: 14.0, 2: 12.0, 3: 16.0, 4: 14.0, 5: 14.0, 6: 7.0, 7: 10.0, 8: 8.0, 9: 25.0, 10: 9.0, 11: 6.0, 12: 23.0, 13: 26.0, 14: 30.0, 15: 18.0, 16: 10.0, 17: 15.0, 18: 14.0, 19: 9.0, 20: 31.0, 21: 12.0, 22: 11.0, 23: 10.0, 24: 15.0, 25: 7.0, 26: 18.0, 27: 11.0, 28: 24.0, 29: 24.0}), features=SparseVector(30, {0: 1.4984, 1: 1.6022, 2: 3.6374, 3: 1.8178, 4: 3.8351, 5: 1.4563, 6: 1.8276, 7: 2.303, 8: 2.0251, 9: 3.3831, 10: 2.0504, 11: 1.5915, 12: 3.6493, 13: 0.8442, 14: 4.9855, 15: 5.0827, 16: 2.0675, 17: 2.9659, 18: 3.0862, 19: 3.073, 20: 1.6507, 21: 2.9922, 22: 2.9849, 23: 1.7727, 24: 3.6272, 25: 2.2298, 26: 3.0017, 27: 2.9106, 28: 2.1264, 29: 5.0347}))\n",
      "Row(label='2', text='I want to give this place 2 stars, but because of the excellent service I\\'ll bump it up to 3. The best thing about this place was the impeccable service received.  I dined here with my husband on a late Friday night.  The food here was just \"OK\". My breakdown of each dish is this:\\n\\nThe Popovers: meh..they were ok. I think they were over cooked which made them a bit too dry and with a slightly burnt butter taste. I like Tyler Florence does a better version at Wayfare Tavern (SF).\\n\\nAppetizer special - Pappardelle w/ Crimini Mushrooms in a white wine sauce finished with parmigiana: Good stuff. Very light and tasty pasta, too many mushrooms for my liking, I would\\'ve preferred more pasta. Very small tasting plate so don\\'t expect to get more than 3 or 4 bites.\\n\\nCreamed spinach: Another meh dish. I wanted more flavor here. Maybe a finish with butter would\\'ve helped. Some seasoning or something. Literally all I could taste was watery spinach.\\n\\nTwice Baled loaded Potatoes: YESSSSSS PLEASE! Now this was a great dish. Flavor in every bite. Perfect partner for my steak.\\n\\nRibeye Steak (w/o bone) OSCAR STYLE w/ Jumbo Lump Crab Meat, Grilled Asparagus and Béarnaise.- I SOOOOOO wanted to love this steak. I really did. Order a my steak cooked medium and unfortunately it came out damn near rare. Like literally just short of bleeding. There was hardly any flavor.  The best part was the burnt edges as they actually added some flavor. I\\'d give my steak a 2/5, however, the Jumbo Lump crab and Béarnaise WAS THE BOMB!\\n\\nSmoked Pork chop: my husband truly enjoyed his pork chop. Tender with a smokey flavor all the way thru. The pork chop was huge! I\\'m not a big pork chop eater, but it was tasty. \\n\\nPecan pie: Now this was worth the price of admission. A tasty warm & nutty pie with cold delicious ice cream. Mmmmm mmmmm good. I\\'d come here again just to eat this. A MUST have if you\\'re ordering desserts.\\n\\nAs previously mentioned in other reviews, the wine here is definitely over priced. I understand the mark ups restaurants need to make, but I think Delmonico\\'s takes the \"upcharge\" to an entirely new level. We ordered a bottle of Kim Crawford\\'s Sauvignon Blanc, which retails for about $18-20, we were charged $45 for the bottle!!! Talk about a rip off! Outside of opening the bottle and pouring the wine, what constitutes a $25 mark up?\\n\\nNeedless to say I most likely won\\'t return here. I\\'m glad I tried it, but I don\\'t think it was worth the price tag. I\\'d prefer to go back to Mario Batali\\'s B&B, which is located directly across the walk way from Delmonico\\'s.', words=['i', 'want', 'to', 'give', 'this', 'place', '2', 'stars,', 'but', 'because', 'of', 'the', 'excellent', 'service', \"i'll\", 'bump', 'it', 'up', 'to', '3.', 'the', 'best', 'thing', 'about', 'this', 'place', 'was', 'the', 'impeccable', 'service', 'received.', '', 'i', 'dined', 'here', 'with', 'my', 'husband', 'on', 'a', 'late', 'friday', 'night.', '', 'the', 'food', 'here', 'was', 'just', '\"ok\".', 'my', 'breakdown', 'of', 'each', 'dish', 'is', 'this:', '', 'the', 'popovers:', 'meh..they', 'were', 'ok.', 'i', 'think', 'they', 'were', 'over', 'cooked', 'which', 'made', 'them', 'a', 'bit', 'too', 'dry', 'and', 'with', 'a', 'slightly', 'burnt', 'butter', 'taste.', 'i', 'like', 'tyler', 'florence', 'does', 'a', 'better', 'version', 'at', 'wayfare', 'tavern', '(sf).', '', 'appetizer', 'special', '-', 'pappardelle', 'w/', 'crimini', 'mushrooms', 'in', 'a', 'white', 'wine', 'sauce', 'finished', 'with', 'parmigiana:', 'good', 'stuff.', 'very', 'light', 'and', 'tasty', 'pasta,', 'too', 'many', 'mushrooms', 'for', 'my', 'liking,', 'i', \"would've\", 'preferred', 'more', 'pasta.', 'very', 'small', 'tasting', 'plate', 'so', \"don't\", 'expect', 'to', 'get', 'more', 'than', '3', 'or', '4', 'bites.', '', 'creamed', 'spinach:', 'another', 'meh', 'dish.', 'i', 'wanted', 'more', 'flavor', 'here.', 'maybe', 'a', 'finish', 'with', 'butter', \"would've\", 'helped.', 'some', 'seasoning', 'or', 'something.', 'literally', 'all', 'i', 'could', 'taste', 'was', 'watery', 'spinach.', '', 'twice', 'baled', 'loaded', 'potatoes:', 'yessssss', 'please!', 'now', 'this', 'was', 'a', 'great', 'dish.', 'flavor', 'in', 'every', 'bite.', 'perfect', 'partner', 'for', 'my', 'steak.', '', 'ribeye', 'steak', '(w/o', 'bone)', 'oscar', 'style', 'w/', 'jumbo', 'lump', 'crab', 'meat,', 'grilled', 'asparagus', 'and', 'béarnaise.-', 'i', 'soooooo', 'wanted', 'to', 'love', 'this', 'steak.', 'i', 'really', 'did.', 'order', 'a', 'my', 'steak', 'cooked', 'medium', 'and', 'unfortunately', 'it', 'came', 'out', 'damn', 'near', 'rare.', 'like', 'literally', 'just', 'short', 'of', 'bleeding.', 'there', 'was', 'hardly', 'any', 'flavor.', '', 'the', 'best', 'part', 'was', 'the', 'burnt', 'edges', 'as', 'they', 'actually', 'added', 'some', 'flavor.', \"i'd\", 'give', 'my', 'steak', 'a', '2/5,', 'however,', 'the', 'jumbo', 'lump', 'crab', 'and', 'béarnaise', 'was', 'the', 'bomb!', '', 'smoked', 'pork', 'chop:', 'my', 'husband', 'truly', 'enjoyed', 'his', 'pork', 'chop.', 'tender', 'with', 'a', 'smokey', 'flavor', 'all', 'the', 'way', 'thru.', 'the', 'pork', 'chop', 'was', 'huge!', \"i'm\", 'not', 'a', 'big', 'pork', 'chop', 'eater,', 'but', 'it', 'was', 'tasty.', '', '', 'pecan', 'pie:', 'now', 'this', 'was', 'worth', 'the', 'price', 'of', 'admission.', 'a', 'tasty', 'warm', '&', 'nutty', 'pie', 'with', 'cold', 'delicious', 'ice', 'cream.', 'mmmmm', 'mmmmm', 'good.', \"i'd\", 'come', 'here', 'again', 'just', 'to', 'eat', 'this.', 'a', 'must', 'have', 'if', \"you're\", 'ordering', 'desserts.', '', 'as', 'previously', 'mentioned', 'in', 'other', 'reviews,', 'the', 'wine', 'here', 'is', 'definitely', 'over', 'priced.', 'i', 'understand', 'the', 'mark', 'ups', 'restaurants', 'need', 'to', 'make,', 'but', 'i', 'think', \"delmonico's\", 'takes', 'the', '\"upcharge\"', 'to', 'an', 'entirely', 'new', 'level.', 'we', 'ordered', 'a', 'bottle', 'of', 'kim', \"crawford's\", 'sauvignon', 'blanc,', 'which', 'retails', 'for', 'about', '$18-20,', 'we', 'were', 'charged', '$45', 'for', 'the', 'bottle!!!', 'talk', 'about', 'a', 'rip', 'off!', 'outside', 'of', 'opening', 'the', 'bottle', 'and', 'pouring', 'the', 'wine,', 'what', 'constitutes', 'a', '$25', 'mark', 'up?', '', 'needless', 'to', 'say', 'i', 'most', 'likely', \"won't\", 'return', 'here.', \"i'm\", 'glad', 'i', 'tried', 'it,', 'but', 'i', \"don't\", 'think', 'it', 'was', 'worth', 'the', 'price', 'tag.', \"i'd\", 'prefer', 'to', 'go', 'back', 'to', 'mario', \"batali's\", 'b&b,', 'which', 'is', 'located', 'directly', 'across', 'the', 'walk', 'way', 'from', \"delmonico's.\"], rawFeatures=SparseVector(30, {0: 31.0, 1: 14.0, 2: 12.0, 3: 16.0, 4: 14.0, 5: 14.0, 6: 7.0, 7: 10.0, 8: 8.0, 9: 25.0, 10: 9.0, 11: 6.0, 12: 23.0, 13: 26.0, 14: 30.0, 15: 18.0, 16: 10.0, 17: 15.0, 18: 14.0, 19: 9.0, 20: 31.0, 21: 12.0, 22: 11.0, 23: 10.0, 24: 15.0, 25: 7.0, 26: 18.0, 27: 11.0, 28: 24.0, 29: 24.0}), features=SparseVector(30, {0: 1.4984, 1: 1.6022, 2: 3.6374, 3: 1.8178, 4: 3.8351, 5: 1.4563, 6: 1.8276, 7: 2.303, 8: 2.0251, 9: 3.3831, 10: 2.0504, 11: 1.5915, 12: 3.6493, 13: 0.8442, 14: 4.9855, 15: 5.0827, 16: 2.0675, 17: 2.9659, 18: 3.0862, 19: 3.073, 20: 1.6507, 21: 2.9922, 22: 2.9849, 23: 1.7727, 24: 3.6272, 25: 2.2298, 26: 3.0017, 27: 2.9106, 28: 2.1264, 29: 5.0347}))\n",
      "Row(label='0', text='Went here for guys weekend. Unbelievable. Ravioli app was so good. The steak was very good too. When getting a good bottle of wine, make sure they decant it for you...had to ask.', words=['went', 'here', 'for', 'guys', 'weekend.', 'unbelievable.', 'ravioli', 'app', 'was', 'so', 'good.', 'the', 'steak', 'was', 'very', 'good', 'too.', 'when', 'getting', 'a', 'good', 'bottle', 'of', 'wine,', 'make', 'sure', 'they', 'decant', 'it', 'for', 'you...had', 'to', 'ask.'], rawFeatures=SparseVector(30, {0: 1.0, 1: 2.0, 3: 1.0, 4: 3.0, 5: 2.0, 8: 3.0, 10: 1.0, 11: 1.0, 13: 1.0, 14: 3.0, 15: 2.0, 16: 1.0, 18: 2.0, 19: 2.0, 20: 1.0, 22: 2.0, 24: 1.0, 26: 3.0, 28: 1.0}), features=SparseVector(30, {0: 0.0483, 1: 0.2289, 3: 0.1136, 4: 0.8218, 5: 0.208, 8: 0.7594, 10: 0.2278, 11: 0.2652, 13: 0.0325, 14: 0.4985, 15: 0.5647, 16: 0.2068, 18: 0.4409, 19: 0.6829, 20: 0.0532, 22: 0.5427, 24: 0.2418, 26: 0.5003, 28: 0.0886}))\n",
      "Row(label='1', text='Went here for guys weekend. Unbelievable. Ravioli app was so good. The steak was very good too. When getting a good bottle of wine, make sure they decant it for you...had to ask.', words=['went', 'here', 'for', 'guys', 'weekend.', 'unbelievable.', 'ravioli', 'app', 'was', 'so', 'good.', 'the', 'steak', 'was', 'very', 'good', 'too.', 'when', 'getting', 'a', 'good', 'bottle', 'of', 'wine,', 'make', 'sure', 'they', 'decant', 'it', 'for', 'you...had', 'to', 'ask.'], rawFeatures=SparseVector(30, {0: 1.0, 1: 2.0, 3: 1.0, 4: 3.0, 5: 2.0, 8: 3.0, 10: 1.0, 11: 1.0, 13: 1.0, 14: 3.0, 15: 2.0, 16: 1.0, 18: 2.0, 19: 2.0, 20: 1.0, 22: 2.0, 24: 1.0, 26: 3.0, 28: 1.0}), features=SparseVector(30, {0: 0.0483, 1: 0.2289, 3: 0.1136, 4: 0.8218, 5: 0.208, 8: 0.7594, 10: 0.2278, 11: 0.2652, 13: 0.0325, 14: 0.4985, 15: 0.5647, 16: 0.2068, 18: 0.4409, 19: 0.6829, 20: 0.0532, 22: 0.5427, 24: 0.2418, 26: 0.5003, 28: 0.0886}))\n"
     ]
    }
   ],
   "source": [
    "# display the generated tfidf \n",
    "print(\"tfidf:\")\n",
    "count = 0\n",
    "for each in rescaledData.collect():    \n",
    "    print(each)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLabeledPoint(line):\n",
    "    return LabeledPoint(line[0],line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 120, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-57-5558219f68f2>\", line 2, in <lambda>\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/regression.py\", line 54, in __init__\n    self.features = _convert_to_vector(features)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-57-5558219f68f2>\", line 2, in <lambda>\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/regression.py\", line 54, in __init__\n    self.features = _convert_to_vector(features)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5558219f68f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make prediction and test accuracy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, lambda_)\u001b[0m\n\u001b[1;32m    671\u001b[0m           \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \"\"\"\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`data` should be an RDD of LabeledPoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Wenonah/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Wenonah/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 120, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-57-5558219f68f2>\", line 2, in <lambda>\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/regression.py\", line 54, in __init__\n    self.features = _convert_to_vector(features)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/rddsampler.py\", line 95, in func\n    for obj in iterator:\n  File \"<ipython-input-57-5558219f68f2>\", line 2, in <lambda>\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/regression.py\", line 54, in __init__\n    self.features = _convert_to_vector(features)\n  File \"/Users/Wenonah/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "print(type(rescaledData))\n",
    "data = rescaledData.rdd.map(lambda item : LabeledPoint(item.label,item.features))\n",
    "training, test = data.randomSplit([0.6, 0.4],seed = 42)\n",
    "\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = test.map(lambda p: [model.predict(p.features), p.label])\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda item: item[0] == item[1]).count() / test.count()\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myNaiveBayesModel\")\n",
    "sameModel = NaiveBayesModel.load(sc, \"target/tmp/myNaiveBayesModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# groupby cat\n",
    "\n",
    "# get one \n",
    "\n",
    "# dup the data \n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(merged_review)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.transform(wordsData).show(truncate=False)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "\n",
    "'''\n",
    "loop through the cat   \n",
    "    groupby(cat).tf\n",
    "\n",
    "# tf idf\n",
    "hashingTF = HashingTF(100)\n",
    "tf = hashingTF.transform(review.map(extrat the text line ))\n",
    "# get it back to the data?\n",
    "\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)\n",
    "'''\n",
    "\n",
    "# Split positive and negative data 60/40 into training and test data sets\n",
    "train, test = review.randomSplit([0.6, 0.4])\n",
    "\n",
    "# Train a Naive Bayes model on the training data\n",
    "model = NaiveBayes.train(train)\n",
    "\n",
    "# Compare predicted labels to actual labels\n",
    "prediction_and_labels = test.map(lambda point: (model.predict(point.features), point.label))\n",
    "\n",
    "# Filter to only correct predictions\n",
    "#correct = prediction_and_labels.filter(lambda(predicted, actual): predicted == actual)\n",
    "\n",
    "# Calculate and print accuracy rate\n",
    "accuracy = correct.count() / float(testh.count())\n",
    "\n",
    "print(\"Classifier correctly predicted category \" + str(accuracy * 100) + \" percent of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
